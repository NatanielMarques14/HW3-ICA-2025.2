{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, C=1.0):\n",
    "        # C = Termo de penalidade (Controla o trade-off entre margem e erro de treinamento)\n",
    "        # Em otimização restrita, atua de forma similar a um Multiplicador de Lagrange\n",
    "        self.C = C\n",
    "        self.w = None # Vetor de pesos (weights)\n",
    "        self.b = None # Viés (bias)\n",
    "        \n",
    "    def calcular_funcao_custo(self, w, b, X, y):\n",
    "        \"\"\"\n",
    "        Calcula a 'Hinge Loss' + Regularização.\n",
    "        Isso representa a Função Objetivo que queremos minimizar.\n",
    "        \"\"\"\n",
    "        # Termo de Regularização (Maximizar a margem geometricamente)\n",
    "        # 0.5 * ||w||^2\n",
    "        regularizacao = 0.5 * np.sum(w ** 2)\n",
    "\n",
    "        # Cálculo vetorial dos \"Resíduos\"\n",
    "        # y_i * (w . x_i + b)\n",
    "        distancia_funcional = y * (np.dot(X, w.T).flatten() + b)\n",
    "        \n",
    "        # Hinge Loss: max(0, 1 - y_i(w.x_i + b))\n",
    "        # Penaliza apenas se o ponto estiver dentro da margem ou classificado errado\n",
    "        perda_hinge = np.maximum(0, 1 - distancia_funcional)\n",
    "        \n",
    "        # Custo Total = Regularização + C * Soma dos Erros\n",
    "        custo_total = regularizacao + self.C * np.sum(perda_hinge)\n",
    "        return custo_total\n",
    "\n",
    "    def fit(self, X, y, tamanho_lote=100, taxa_aprendizado=0.001, epocas=1000):\n",
    "        \"\"\"\n",
    "        Treina o modelo usando Descida do Gradiente (Gradient Descent - Steepest Descent).\n",
    "        \"\"\"\n",
    "        n_amostras, n_features = X.shape\n",
    "\n",
    "        # Inicialização dos parâmetros (w e b)\n",
    "        self.w = np.zeros((1, n_features))\n",
    "        self.b = 0\n",
    "        \n",
    "        historico_custo = []\n",
    "\n",
    "        # Loop de Otimização (Iterações/Épocas)\n",
    "        for i in range(epocas):\n",
    "            \n",
    "            # Embaralhamento dos dados (Shuffling)\n",
    "            # Essencial para a convergência do Gradiente\n",
    "            indices = np.arange(n_amostras)\n",
    "            np.random.shuffle(indices)\n",
    "            X_embaralhado = X[indices]\n",
    "            y_embaralhado = y[indices]\n",
    "\n",
    "            # Monitoramento da Função Objetivo\n",
    "            custo_atual = self.calcular_funcao_custo(self.w, self.b, X, y)\n",
    "            historico_custo.append(custo_atual)\n",
    "\n",
    "            # Iteração sobre Batch\n",
    "            for inicio in range(0, n_amostras, tamanho_lote):\n",
    "                fim = inicio + tamanho_lote\n",
    "                X_batch = X_embaralhado[inicio:fim]\n",
    "                y_batch = y_embaralhado[inicio:fim]\n",
    "\n",
    "                # --- Cálculo do Gradiente (Derivadas Parciais) ---\n",
    "                \n",
    "                # Verificamos a condição da margem: y * (w.x + b) < 1\n",
    "                condicao = y_batch * (np.dot(X_batch, self.w.T).flatten() + self.b)\n",
    "                \n",
    "                # Identifica índices onde o modelo errou ou violou a margem\n",
    "                indices_violacao = np.where(condicao < 1)[0]\n",
    "\n",
    "                # 1. Gradiente da Regularização (sempre aplicado)\n",
    "                # Derivada de 0.5*w^2 é w\n",
    "                grad_w = self.w \n",
    "                grad_b = 0\n",
    "\n",
    "                # 2. Gradiente da Perda Hinge (aplicado apenas onde há violação)\n",
    "                if len(indices_violacao) > 0:\n",
    "                    X_violacao = X_batch[indices_violacao]\n",
    "                    y_violacao = y_batch[indices_violacao].reshape(-1, 1)\n",
    "\n",
    "                    # Derivada parcial em relação a w: -C * y * x\n",
    "                    grad_w_erro = -self.C * np.dot(y_violacao.T, X_violacao)\n",
    "                    \n",
    "                    # Derivada parcial em relação a b: -C * y\n",
    "                    grad_b_erro = -self.C * np.sum(y_violacao)\n",
    "                    \n",
    "                    # Soma os componentes do gradiente (Regularização + Erro)\n",
    "                    # Nota: Como a atualização abaixo é w = w - lr * grad,\n",
    "                    # e o termo do erro é negativo, somamos ele aqui para que a subtração\n",
    "                    # na atualização resulte em uma soma (descida correta).\n",
    "                    # A lógica expandida seria: w_novo = w_velho - taxa * (w_velho - C*y*x)\n",
    "                    \n",
    "                    # Atualização dos Pesos (Descida do Gradiente)\n",
    "                    self.w = self.w - taxa_aprendizado * (self.w + grad_w_erro)\n",
    "                    self.b = self.b - taxa_aprendizado * grad_b_erro\n",
    "                \n",
    "                else:\n",
    "                    # Se não houver violação, apenas o termo de regularização atua (decaimento de peso)\n",
    "                    self.w = self.w - taxa_aprendizado * self.w\n",
    "\n",
    "        return self.w, self.b, historico_custo\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predição baseada no sinal da projeção\n",
    "        predicao = np.dot(X, self.w.T) + self.b\n",
    "        return np.sign(predicao).flatten()\n",
    "\n",
    "# --- Visualização ---\n",
    "\n",
    "def visualizar_svm(modelo, X, y):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter', s=50, alpha=0.6)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    \n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    \n",
    "    # Parâmetros otimizados\n",
    "    w = modelo.w.flatten()\n",
    "    b = modelo.b\n",
    "    \n",
    "    # Hiperplano de Decisão (Decision Boundary): w.x + b = 0\n",
    "    yy = (-w[0] * xx - b) / w[1]\n",
    "    \n",
    "    # Margens (+1 e -1): w.x + b = 1  e  w.x + b = -1\n",
    "    margem_pos = (1 - w[0] * xx - b) / w[1]\n",
    "    margem_neg = (-1 - w[0] * xx - b) / w[1]\n",
    "\n",
    "    plt.plot(xx, yy, 'k-', label='Hiperplano de Separação')\n",
    "    plt.plot(xx, margem_pos, 'k--', linewidth=0.5, label='Margem (+1)')\n",
    "    plt.plot(xx, margem_neg, 'k--', linewidth=0.5, label='Margem (-1)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f\"SVM Linear (C={modelo.C})\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Geração de dados sintéticos\n",
    "    X = np.random.randn(200, 2) * 2\n",
    "    # Rótulos baseados em uma separação linear simples\n",
    "    y = np.array([1 if x[0] + x[1] > 0 else -1 for x in X])\n",
    "    \n",
    "    # Instância e Treinamento\n",
    "    svm = SVM(C=1.0)\n",
    "    w_final, b_final, historico = svm.fit(X, y, tamanho_lote=20, epocas=500)\n",
    "    \n",
    "    visualizar_svm(svm, X, y)\n",
    "    \n",
    "    plt.plot(historico)\n",
    "    plt.title(\"Convergência da Função Objetivo (Descida do Gradiente)\")\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Custo\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536c2c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
